{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Image Processing and Analysis with Python  \n",
    "   \n",
    "This notebook provides a comprehensive guide to processing and analyzing images using Python. It leverages various libraries and tools to fetch, manipulate, and analyze images, particularly focusing on extracting and deduplicating image data, as well as integrating with cloud services for data storage and processing.  \n",
    "   \n",
    "## Key Libraries and Tools  \n",
    "   \n",
    "1. **Python Libraries**:  \n",
    "   - `requests`: For fetching images from URLs.  \n",
    "   - `base64`: For encoding image content.  \n",
    "   - `pandas`: For data manipulation and analysis.  \n",
    "   - `PIL` (Python Imaging Library): For image processing.  \n",
    "   - `hashlib`: For computing image hashes.  \n",
    "   - `tqdm`: For progress bars.  \n",
    "   - `dotenv`: For loading environment variables.  \n",
    "   - `google.cloud.bigquery`: For interacting with Google BigQuery.  \n",
    "   \n",
    "2. **LangChain**:  \n",
    "   - `AzureChatOpenAI`: For integrating with Azure OpenAI services.  \n",
    "   - `BaseModel` and `Field` from `langchain_core.pydantic_v1`: For creating structured data models.  \n",
    "   - `HumanMessage` and `chain` from `langchain_core.messages` and `runnables`: For creating and running message chains.  \n",
    "   - `JsonOutputParser`: For parsing JSON outputs.  \n",
    "\n",
    "      \n",
    "## Notebook Workflow  \n",
    "   \n",
    "### 1. Setup and Initialization  \n",
    "The notebook begins by importing necessary libraries and setting up environment variables using `dotenv`. It also configures pandas display options for better readability.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "import os\n",
    "load_dotenv(\".env\", override=True)\n",
    "import base64\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.messages import HumanMessage\n",
    "from typing import Literal, List, Union, Optional\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from google.cloud import bigquery\n",
    "import tqdm\n",
    "\n",
    "from PIL import Image  \n",
    "from io import BytesIO  \n",
    "from IPython import display\n",
    "from IPython.display import Markdown, Video, display\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import hashlib  \n",
    "from tqdm import tqdm \n",
    "from google.cloud import storage\n",
    "from PIL import Image  # Using PIL for image manipulation\n",
    "\n",
    "from tools.octocloud import Dataframe, Table, Directory\n",
    "import time\n",
    "\n",
    "from pydantic import BaseModel, Field, constr, field_validator  \n",
    "from typing_extensions import Literal\n",
    "\n",
    "# Pandas display options\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_colwidth', None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 400\n",
    "\n",
    "# Constants\n",
    "client = bigquery.Client(os.getenv(\"PROJECT_ID\"))\n",
    "directory = Directory(client, os.getenv(\"PROJECT_ID\"), dataset=\"renault_crea\")\n",
    "input_ads_table = Table(client, directory, \"fb_ads_test\")\n",
    "intermediate_image_table = Table(client, directory, \"fb_ads_image_urls_intermediate\")\n",
    "intermediate_video_table = Table(client, directory, \"fb_ads_video_urls_intermediate\")\n",
    "output_image_table = Table(client, directory, \"fb_ads_image_final\")\n",
    "output_video_table = Table(client, directory, \"fb_ads_video_final\")\n",
    "\n",
    "# Clean tables used for dashboard\n",
    "features_image_table = Table(client, directory, \"fb_ads_image_features\")\n",
    "features_video_table = Table(client, directory, \"fb_ads_video_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_image = \"\"\" \n",
    "SELECT\n",
    "  *\n",
    "FROM(\n",
    "  SELECT \n",
    "    id, \n",
    "    category,\n",
    "    image_url, \n",
    "  FROM {input_table} LEFT JOIN UNNEST(image_urls) AS image_url\n",
    ")\n",
    "WHERE image_url IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "sql_video = \"\"\" \n",
    "SELECT\n",
    "  *\n",
    "FROM(\n",
    "  SELECT \n",
    "    id, \n",
    "    category,\n",
    "    video_url, \n",
    "  FROM {input_table} LEFT JOIN UNNEST(video_urls) AS video_url \n",
    ")\n",
    "WHERE video_url IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "query_image = sql_image.format(input_table=input_ads_table.path(\"standard\"))\n",
    "query_video = sql_video.format(input_table=input_ads_table.path(\"standard\"))\n",
    "df_image = client.query_and_wait(query_image).to_dataframe()\n",
    "df_video = client.query_and_wait(query_video).to_dataframe()\n",
    "print(\"Number of images:\", len(df_image))\n",
    "print(\"Number of images:\", len(df_video))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the size of the sample in test phase\n",
    "df_test_image = df_image.head(SAMPLE_SIZE).copy()\n",
    "df_test_video = df_video.head(round(SAMPLE_SIZE/4)).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Image processing\n",
    "#### Data cleaning: Image deduplication and perceptual hashing\n",
    "\n",
    "Hashing is the transformation of any data into a usually shorter fixed-length value or key that represents the original string.\n",
    "\n",
    "Just as we have unique Fingerprints, Hashes are unique for any particular data. There are lots of Hashing Algorithms out there which cater to specific needs.\n",
    "\n",
    "Perceptual hash is like a regular hash in that it is a smaller, compare-able fingerprint of a much larger piece of data. However, unlike a typical hashing algorithm, the idea with a perceptual hash is that the perceptual hashes are “close” (or equal) if the original images are close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dhash(image, hash_size=8):\n",
    "    \"\"\"\n",
    "    Generates a dHash for the input image.\n",
    "    Args:\n",
    "        image (PIL.Image.Image): Input image\n",
    "        hash_size: the size to which the image has to be resized for hash calculation\n",
    "    Returns:\n",
    "        str: Hexadecimal representation of the hash.\n",
    "    \"\"\"\n",
    "    resized = image.resize((hash_size + 1, hash_size), Image.LANCZOS).convert('L')\n",
    "    difference = []\n",
    "    for row in range(hash_size):\n",
    "        for col in range(hash_size):\n",
    "            pixel_left = resized.getpixel((col, row))\n",
    "            pixel_right = resized.getpixel((col + 1, row))\n",
    "            difference.append(pixel_left > pixel_right)\n",
    "\n",
    "    # Convert boolean list to integer, then to hex representation\n",
    "    decimal_value = int(\"\".join([str(int(b)) for b in difference]), 2)\n",
    "    hash_hex = hex(decimal_value)[2:]\n",
    "    \n",
    "    # If the hex string has less digits, left pad with zeros\n",
    "    hash_hex_len = hash_size * hash_size / 4 # Divide by 4 to get correct no of hex characters as 4 bits can be represented using 1 hex character.\n",
    "    hash_hex =  hash_hex.zfill(int(hash_hex_len))\n",
    "    \n",
    "    return hash_hex\n",
    "\n",
    "def image_to_bytes(gcs_url):\n",
    "    \"\"\"Loads an image from Google Cloud Storage and returns a PIL Image object.\n",
    "    Args:\n",
    "        gcs_url (str): Full GCS URL of the image (e.g., \"gs://bucket/image.jpg\").\n",
    "    Returns:\n",
    "        PIL.Image.Image: PIL Image object, or None if an error occurred.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        bucket_name, blob_name = gcs_url.replace(\"gs://\", \"\").split(\"/\")[-2:]\n",
    "        client = storage.Client(os.getenv(\"PROJECT_ID\"))\n",
    "        bucket = client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "        image_bytes = blob.download_as_bytes()\n",
    "        return image_bytes\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image from GCS: {e}\")\n",
    "        return None\n",
    "\n",
    "def image_to_pillow(gcs_url):\n",
    "\n",
    "    image = Image.open(BytesIO(image_to_bytes(gcs_url)))\n",
    "    return image\n",
    "\n",
    "    \n",
    "def calculate_aspect(width: int, height: int) -> str:\n",
    "    def gcd(a, b):\n",
    "        \"\"\"The GCD (greatest common divisor) is the highest number that evenly divides both width and height.\"\"\"\n",
    "        return a if b == 0 else gcd(b, a % b)\n",
    "\n",
    "    r = gcd(width, height)\n",
    "    x = int(width / r)\n",
    "    y = int(height / r)\n",
    "\n",
    "    return f\"{x}:{y}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each row in the DataFrame, showing a progress bar  \n",
    "for index, row in tqdm(df_test_image.iterrows(), total=df_test_image.shape[0]):  \n",
    "    gcs_url = row[\"image_url\"]  # Retrieve the image URL from the \"image_url\" column of the current row  \n",
    "    image = image_to_pillow(gcs_url)  # Convert the image URL to a Pillow Image object  \n",
    "  \n",
    "    if image:  \n",
    "        # If the image was successfully loaded, update the corresponding columns in the DataFrame  \n",
    "        df_test_image.loc[index, \"image_hash\"] = dhash(image)  # Calculate the image hash and store it in the \"image_hash\" column  \n",
    "        df_test_image.loc[index, \"size\"] = f\"{image.size[1]}:{image.size[0]}\"  # Store the image size in the \"size\" column  \n",
    "        df_test_image.loc[index, \"aspect_ratio\"] = calculate_aspect(image.size[1], image.size[0])  # Calculate the aspect ratio and store it in the \"aspect_ratio\" column  \n",
    "    else:  \n",
    "        # If the image could not be loaded, update the corresponding columns with None values  \n",
    "        df_test_image.loc[index, \"image_hash\"] = None  \n",
    "        df_test_image.loc[index, \"size\"] = None  \n",
    "        df_test_image.loc[index, \"aspect_ratio\"] = None  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the content of one image\n",
    "\n",
    "image_to_pillow(gcs_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export of intermediate table to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_image.reset_index(inplace=True)\n",
    "Dataframe(client, df_test_image).to_table(intermediate_image_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction from image\n",
    "\n",
    "1. First we retrieve the unique images from the intermediary table (after image deduplication)\n",
    "2. Initialize a GenAI client, which is used to interact with the Gemini 2.0 Flash Experimental model.  \n",
    "3. Define a data model called `ImageInformation` using Pydantic. This model represents the features that will be extracted from each image.  \n",
    "4. Create an empty list called `features` to store the extracted features for each image.  \n",
    "5. Iterating over the dataframe, we make a request to the Gemini 2.0 Flash Experimental model to generate content based on the image.  \n",
    "6. We retrieve the response from the model is parsed as JSON and stored in the `feature_dict` variable.  \n",
    "7. The `feature_dict` is updated with additional information such as the image URL and image hash to join it later onto the original table.\n",
    "11. We export the final list of feature dict to a BigQuery table using the octocloud script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query to retrieve data from the intermediate_image_table  \n",
    "sql = \"\"\"   \n",
    "SELECT     \n",
    "  image_hash,  \n",
    "  MAX(image_url) AS image_url,\n",
    "FROM {intermediate_image_table}  \n",
    "GROUP BY 1\n",
    "\"\"\"  \n",
    "  \n",
    "# Format the SQL query with the table name  \n",
    "query = sql.format(  \n",
    "    intermediate_image_table=intermediate_image_table.path(\"standard\")  \n",
    ")  \n",
    "  \n",
    "# Execute the query and wait for the result, then convert it to a DataFrame  \n",
    "df_deduplicated_images = client.query_and_wait(query).to_dataframe()  \n",
    "  \n",
    "# Print the number of images in the DataFrame  \n",
    "print(\"Number of images:\", len(df_deduplicated_images))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "from pydantic import BaseModel, Field, constr, field_validator  \n",
    "from typing import List, Union\n",
    "from typing_extensions import Literal\n",
    "from typing import Optional\n",
    "import json \n",
    "\n",
    "genai_client = genai.Client(\n",
    "    vertexai=True, project=os.getenv(\"PROJECT_ID\"), location='us-central1'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "class ImageInformation(BaseModel):  \n",
    "    \"\"\"Information about an image.\"\"\"  \n",
    "      \n",
    "    image_description: str = Field(description=\"A brief one-sentence description of the creative.\")  \n",
    "    product_type: Literal[\"car\", \"service\", \"connected services\", \"accessories\"] = Field(description=\"The type of product represented by the advertisement.\")  \n",
    "    contrast_presence: bool = Field(description=\"Is the contrast strong between text and background color? True if 'yes', False if 'no'.\")  \n",
    "    product_elements: List[str] = Field(description=\"List of short descriptions of products in the creative.\")\n",
    "    # meta_text: bool = Field(description=\"Is the text displayed in the first 5 seconds of the video? True if 'yes', False if 'no'.\") \n",
    "    # meta_video_length: bool = Field(description=\"Is the video duration more than 15 seconds ? True if 'yes', False if 'no'.\") \n",
    "    # meta_product_presence: bool = Field(description=\"Is the product (car) displayed in the first 3 seconds of the video? True if 'yes', False if 'no'.\") \n",
    "    product_presence: bool = Field(description=\"Is the car displayed in the ad? True if 'yes', False if 'no'.\")  \n",
    "    # product_share: int = Field(default = None, description=\"Percentage of video duration where the product is displayed\")  \n",
    "    product_percent: int = Field(description=\"Percentage of the creative area covered by the product.\")  \n",
    "    main_objects: List[str] = Field(description=\"List of the main objects in the picture.\")  \n",
    "    text: bool = Field(description=\"Indicates if there is text in the creative. True if 'yes', False if 'no'.\")  \n",
    "    text_location: Literal[\"top\", \"down\", \"right\", \"left\", \"center\", \"None\"]= Field(default=\"None\", description=\"Locations ad related text that is not the call to action\")  \n",
    "    brand_logo: bool = Field(description=\"Indicates if there is a brand logo on the car. True if 'yes', False if 'no'.\")  \n",
    "    promotion: bool = Field(description=\"Indicates if there is a promotion call in the creative. True if 'yes', False if 'no'.\")  \n",
    "    promotion_deadline: bool = Field(description=\"Indicates if a promotion deadline is displayed in the creative. True if 'yes', False if 'no'.\")  \n",
    "    promotion_theme: str= Field(default= \"None\", description=\"Specifies if the promotion is linked to a special event (e.g., Black Friday, Christmas).\")  \n",
    "    call_to_action: bool = Field(description=\"Indicates if there is a short call to action in the creative. The call to action should include a verb and it's length should have a maximum of 4 words. True if 'yes', False if 'no'.\")  \n",
    "    call_to_action_size: int = Field(default=None, description=\"Percentage of the creative area covered by the call to action, if applicable.\")  \n",
    "    car_brand: str = Field(default=\"None\", description=\"The brand of the car, only if it is displayed on the product itself.\")  \n",
    "    car_model: str = Field(default=\"None\", description=\"The model of the car, if applicable.\")  \n",
    "    price: bool = Field(description=\"Indicates if a price is displayed in the creative. True if 'yes', False if 'no'.\")  \n",
    "    ad_purpose: str = Field(default=\"awareness\", description=\"The intended action promoted by the verb mentionned in the text (e.g., buy, order, discover, test).\")  \n",
    "    text_extraction: str = Field(default=\"None\", description=\"The text content extracted from the creative.\")  \n",
    "    price_size: int = Field(default=\"None\", description=\"Percentage of the creative area covered by the price, if applicable.\") \n",
    "    price_type: Literal[\"None\", \"net price\", \"recurring monthly\"] = Field(default=\"None\", description=\"The format in which the price is presented. This can either be a one-time full net price or a recurring monthly price.\") \n",
    "    color_palette: List[str] = Field(description=\"Dominant colors used in the creative.\")  \n",
    "    human_elements: List[str] = Field(default=\"None\", description=\"Short descriptions of main humans in the creative.\")  \n",
    "    human_presence: bool = Field(description=\"Are there any distinguishable humans displayed in the ad? True if 'yes', False if 'no'.\")  \n",
    "    human_number: int = Field(default=\"None\", description=\"Number of distinguishable humans in the creative.\")  \n",
    "    human_genders: List[str] = Field(default=\"None\", description=\"List of humans' gender in the picture.\")  \n",
    "    imagery_type: Literal[\"photographs\", \"illustration\", \"mix media\", \"infographics\", \"3D rendering\", \"icons\"] = Field(description=\"Type of imagery in the creative.\")  \n",
    "    style: List[str] = Field(description=\"General aesthetic and style of the creative (e.g., modern, vintage, minimalist, abstract).\")  \n",
    "    actions: List[str] = Field(description=\"Activities or actions taking place in the creative (e.g., running, jumping, eating).\")  \n",
    "    actions_presence: bool = Field(description=\"Is there an action/story taking place in the ad? True if 'yes', False if 'no'.\")  \n",
    "    environment: List[Literal[\"indoor\", \"outdoor\", \"urban\", \"rural\", \"None\"]] = Field(description=\"The environment in which the scene is set.\")  \n",
    "    time_of_day: Literal[\"not identifiable\", \"daytime\", \"nighttime\", \"sunset\", \"dawn\"] = Field(description=\"The time of day in the creative.\")  \n",
    "    weather: Literal[\"None\", \"sunny\", \"cloudy\", \"rainy\"] = Field(default=\"None\", description=\"The weather condition in the creative, if identifiable.\")  \n",
    "    background_main_color: str = Field(default=\"None\", description=\"Dominant color of the background.\")  \n",
    "    human_shot_size: List[str] = Field(default=\"None\", description=\"Shot size of the human in the creative (e.g., full shot, medium full shot, cowboy shot, closeup).\")  \n",
    "    human_age: Literal[\"senior\", \"adult\", \"child\", \"baby\"] = Field(default=\"None\", description=\"The age group of the human(s) in the  if they are distinguishable.\")\n",
    "    electric_vehicle: bool = Field(description=\"Is the ad related to the electrification of the vehicle? True if 'yes', False if 'no'.\")  \n",
    "    goal_awareness_or_consideration_or_conversion: Literal[\"awareness\", \"consideration\", \"conversion\"] = Field(default=\"None\", description=\"The ad's purpose based on the call to action, estimation is linked to consideration, while configuration is linked to consideration.\") \n",
    "    # meta_logo_display: bool = Field(description=\"Is the brand referenced in the first 3 seconds of the video, without considering the logo on the product? True if 'yes', False if 'no'.\")\n",
    "    # branding_share: Optional[int] = Field(None, description=\"Percentage of the video where Renault branding (logo, text, etc.) is shown.\")  \n",
    "    black_bars: bool = Field(description=\"Is there a black bar on the creative? True if 'yes', False if 'no'.\")  \n",
    "    text_length: int = Field(default = None, description=\"Number of lines the text takes on the creative without considering the call to action.\") \n",
    "    promotion_display: bool = Field(description=\"Indicates whether a price discount, or price promotion message is displayed in the ad. True if 'yes', False if 'no'.\")  \n",
    "    call_to_action_verb: str = Field(default=\"None\", description=\"The specific verb used in the call to action, if present (e.g., 'buy', 'test').\")  \n",
    "    promotion_wording_type: Literal[\"None\", \"gain\", \"loss_aversion\"] = Field(default=\"None\", description=\"Specifies the type of promotion wording used in the ad: 'gain' for emphasizing benefits, 'loss_aversion' for emphasizing avoidance of loss, or 'None' if there is no promotion wording.\")  \n",
    "    persuasion: Literal[\"None\", \"informative\", \"persuasive\", \"emotional\", \"humorous\", \"inspirational\", \"authoritative\", \"urgent\", \"relatable\"] = Field(default=\"None\", description=\"The tone of voice used for the advertisement, if applicable (e.g., Humour, Surprise, Suspense, Love, Happiness, Neutral).\")  \n",
    "    new_old_vehicle: Literal[\"None\", \"new\", \"second-hand\"] = Field(default=\"None\", description=\"Indicates whether the advertised vehicle is new or old.\")  \n",
    "    # audio_presence: Optional[bool] = Field(None, description=\"Is there audio (music, voice, ...) in the video? True if 'yes', False if 'no'.\")  \n",
    "    # subtitles_presence: Optional[bool] = Field(None, description=\"Are there subtitles in the video? True if 'yes', False if 'no'.\")  \n",
    "    product_usage: bool = Field(description=\"Is the product (car, connected feature, etc.) used in the Ad? Is there a creative of someone driving the car? True if 'yes', False if 'no'.\")  \n",
    "    # action_audio: bool = Field(description=\"Is there a clear audio voice that is delivering the call to action? True if 'yes', False if 'no'.\") \n",
    "\n",
    "def retry(fun, max_tries=3, sleep=1):\n",
    "    for i in range(max_tries):\n",
    "        try:\n",
    "           fun()\n",
    "           break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(sleep) \n",
    "            continue\n",
    "\n",
    "features_image = []\n",
    "\n",
    "\n",
    "def append_features_image():\n",
    "\n",
    "    # Load the image from GCS\n",
    "    gcs_url = row[\"image_url\"]\n",
    "    bytes = image_to_bytes(gcs_url)\n",
    "\n",
    "    response = genai_client.models.generate_content(\n",
    "        model='gemini-2.0-flash-exp', \n",
    "        contents=[\n",
    "            'Describe the content of the picture',\n",
    "            types.Part.from_bytes(data=bytes, mime_type=\"image/png\")    \n",
    "        ],\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=0,\n",
    "            response_mime_type= 'application/json',\n",
    "            response_schema= ImageInformation\n",
    "        )\n",
    "    )\n",
    "\n",
    "    feature_dict = json.loads(response.text)\n",
    "    feature_dict.update({\"image_url\": row[\"image_url\"], \"image_hash\": row[\"image_hash\"]})\n",
    "    features_image.append(feature_dict)\n",
    "    return response\n",
    "\n",
    "for index, row  in tqdm(df_deduplicated_images.iterrows(), total=df_deduplicated_images.shape[0]):\n",
    "    response = retry(append_features_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features post processing\n",
    "\n",
    "Cleaning of the extracted features in order to homogenize the content. This includes:\n",
    "- lowercase\n",
    "- uniformization of null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of dict into a dataframe\n",
    "output_image_features_df = pd.DataFrame(features_image)\n",
    "output_image_features_df.reset_index(inplace=True, drop=True)\n",
    "output_image_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert all text columns to lower case  \n",
    "for col in output_image_features_df.select_dtypes(include='object').columns:  \n",
    "    output_image_features_df[col] = output_image_features_df[col].apply(lambda x: x.lower() if isinstance(x, str) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace occurrences of the string \"None\" with np.nan  \n",
    "output_image_features_df.replace('none', np.nan, inplace=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image_features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export results to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to BQ\n",
    "Dataframe(client, output_image_features_df).to_table(output_image_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate costs\n",
    "\n",
    "Cost estimation are base on the following references:\n",
    "- [Google cloud pricing list](https://cloud.google.com/skus?hl=en&filter=gemini&currency=USD)\n",
    "- [Vertex AI pricing list](https://cloud.google.com/vertex-ai/generative-ai/pricing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_url = df_deduplicated_images.iloc[0][\"image_url\"]\n",
    "bytes = image_to_bytes(gcs_url)\n",
    "\n",
    "response = genai_client.models.generate_content(\n",
    "    model='gemini-2.0-flash-exp', \n",
    "    contents=[\n",
    "        'Describe the content of the picture',\n",
    "        types.Part.from_bytes(data=bytes, mime_type=\"image/png\")    \n",
    "    ],\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=0,\n",
    "        response_mime_type= 'application/json',\n",
    "        response_schema= ImageInformation\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of input tokens, output tokens, and cached tokens from the response  \n",
    "input_tokens = response.usage_metadata.prompt_token_count  \n",
    "output_tokens = response.usage_metadata.candidates_token_count  \n",
    "intermediate_tokens = response.usage_metadata.cached_content_token_count  \n",
    "  \n",
    "# If there are cached tokens, print the count  \n",
    "if intermediate_tokens:  \n",
    "    print(\"Cached tokens:\", intermediate_tokens)  \n",
    "  \n",
    "# Print the count of input tokens and output tokens  \n",
    "print(\"Input tokens:\", input_tokens)  \n",
    "print(\"Output tokens:\", output_tokens)  \n",
    "  \n",
    "# Get the number of images from the dataframe  \n",
    "NUMBER_IMAGE = len(df_deduplicated_images)  \n",
    "  \n",
    "# Calculate the price based on the number of tokens  \n",
    "price = input_tokens / 1E3 * 0.00001875 + output_tokens / 1E3 * 0.000075  \n",
    "  \n",
    "# Set the price per image  \n",
    "image_price = 0.00002  \n",
    "  \n",
    "# Calculate the total price for all tokens and images  \n",
    "total_price = (price + image_price) * NUMBER_IMAGE  \n",
    "  \n",
    "# Print the total price formatted as dollars with two decimal places  \n",
    "print(f\"Total estimated cost for {NUMBER_IMAGE} images: ${total_price}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Video Processing\n",
    "\n",
    "#### Video Preprocessing\n",
    "First we extract the size and aspect ratio of the video and send the information into an intermediary table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each row in the DataFrame, showing a progress bar  \n",
    "for index, row in tqdm(df_test_video.iterrows(), total=df_test_video.shape[0]): \n",
    "\n",
    "    gcs_url = row[\"video_url\"]  # Retrieve the image URL from the \"image_url\" column of the current row  \n",
    "    vid = cv2.VideoCapture(gcs_url)\n",
    "    height = vid.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    width = vid.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "  \n",
    "    if vid:  \n",
    "        # If the image was successfully loaded, update the corresponding columns in the DataFrame  \n",
    "        df_test_video.loc[index, \"size\"] = f\"{height}:{width}\"  # Store the image size in the \"size\" column  \n",
    "        df_test_video.loc[index, \"aspect_ratio\"] = calculate_aspect(height, width)  # Calculate the aspect ratio and store it in the \"aspect_ratio\" column  \n",
    "    else:  \n",
    "        # If the image could not be loaded, update the corresponding columns with None values  \n",
    "        df_test_video.loc[index, \"size\"] = None  \n",
    "        df_test_video.loc[index, \"aspect_ratio\"] = None  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the content of a video\n",
    "display(Video(df_test_video[\"video_url\"].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_video.reset_index(inplace=True, drop=True)\n",
    "Dataframe(client, df_test_video).to_table(intermediate_video_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoInformation(BaseModel):  \n",
    "    \"\"\"Information about an image.\"\"\"  \n",
    "      \n",
    "    image_description: str = Field(description=\"A brief one-sentence description of the creative.\")  \n",
    "    product_type: Literal[\"car\", \"service\", \"connected services\", \"accessories\"] = Field(description=\"The type of product represented by the advertisement.\")  \n",
    "    contrast_presence: bool = Field(description=\"Is the contrast strong between text and background color? True if 'yes', False if 'no'.\") \n",
    "    product_elements: List[str] = Field(description=\"List of short descriptions of products in the creative.\") \n",
    "    meta_text: bool = Field(description=\"Is the text displayed in the first 5 seconds of the video? True if 'yes', False if 'no'.\") \n",
    "    meta_video_length: bool = Field(description=\"Is the video duration more than 15 seconds ? True if 'yes', False if 'no'.\") \n",
    "    meta_product_presence: bool = Field(description=\"Is the product (car) displayed in the first 3 seconds of the video? True if 'yes', False if 'no'.\")  \n",
    "    product_presence: bool = Field(description=\"Is the car displayed in the ad? True if 'yes', False if 'no'.\")  \n",
    "    product_share: int = Field(default = None, description=\"Percentage of video duration where the product is displayed\")  \n",
    "    product_percent: int = Field(description=\"Percentage of the creative area covered by the product.\")  \n",
    "    main_objects: List[str] = Field(description=\"List of the main objects in the picture.\")  \n",
    "    text: bool = Field(description=\"Indicates if there is text in the creative. True if 'yes', False if 'no'.\")  \n",
    "    text_location: Literal[\"top\", \"down\", \"right\", \"left\", \"center\", \"None\"]= Field(default=\"None\", description=\"Locations ad related text that is not the call to action\")  \n",
    "    brand_logo: bool = Field(description=\"Indicates if there is a brand logo on the product (car). True if 'yes', False if 'no'.\")  \n",
    "    promotion: bool = Field(description=\"Indicates if there is a promotion call in the creative. True if 'yes', False if 'no'.\")  \n",
    "    promotion_deadline: bool = Field(description=\"Indicates if a promotion deadline is displayed in the creative. True if 'yes', False if 'no'.\")  \n",
    "    promotion_theme: str= Field(default= \"None\", description=\"Specifies if the promotion is linked to a special event (e.g., Black Friday, Christmas).\")  \n",
    "    call_to_action: bool = Field(description=\"Indicates if there is a short call to action in the creative. The call to action should include a verb and it's length should have a maximum of 4 words. True if 'yes', False if 'no'.\")  \n",
    "    call_to_action_size: int = Field(default=None, description=\"Percentage of the creative area covered by the call to action, if applicable.\")  \n",
    "    car_brand: str = Field(default=\"None\", description=\"The brand of the car, only if it is displayed on the product itself.\") \n",
    "    car_model: str = Field(default=\"None\", description=\"The model of the car, if applicable.\")  \n",
    "    price: bool = Field(description=\"Indicates if a price is displayed in the creative. True if 'yes', False if 'no'.\")  \n",
    "    ad_purpose: str = Field(default=\"awareness\", description=\"The intended action promoted by the verb mentionned in the text (e.g., buy, order, discover, test).\")  \n",
    "    text_extraction: str = Field(default=\"None\", description=\"The text content extracted from the creative.\")  \n",
    "    price_size: int = Field(default=\"None\", description=\"Percentage of the creative area covered by the price, if applicable.\")\n",
    "    price_type: Literal[\"None\", \"net price\", \"recurring monthly\"] = Field(default=\"None\", description=\"The format in which the price is presented. This can either be a one-time full net price or a recurring monthly price.\")   \n",
    "    color_palette: List[str] = Field(description=\"Dominant colors used in the creative.\")  \n",
    "    human_elements: List[str] = Field(default=\"None\", description=\"Short descriptions of main humans in the creative.\")  \n",
    "    human_presence: bool = Field(description=\"Are there any distinguishable humans displayed in the ad? True if 'yes', False if 'no'.\") \n",
    "    human_number: int = Field(default=\"None\", description=\"Number of distinguishable humans in the creative.\")  \n",
    "    human_genders: List[str] = Field(default=\"None\", description=\"List of humans' gender in the picture.\")  \n",
    "    imagery_type: Literal[\"photographs\", \"illustration\", \"mix media\", \"infographics\", \"3D rendering\", \"icons\"] = Field(description=\"Type of imagery in the creative.\")  \n",
    "    style: List[str] = Field(description=\"General aesthetic and style of the creative (e.g., modern, vintage, minimalist, abstract).\")  \n",
    "    actions: List[str] = Field(description=\"Activities or actions taking place in the creative (e.g., running, jumping, eating).\")  \n",
    "    actions_presence: bool = Field(description=\"Is there an action/story taking place in the ad? True if 'yes', False if 'no'.\")  \n",
    "    environment: List[Literal[\"indoor\", \"outdoor\", \"urban\", \"rural\", \"None\"]] = Field(description=\"The environment in which the scene is set.\")   \n",
    "    time_of_day: Literal[\"not identifiable\", \"daytime\", \"nighttime\", \"sunset\", \"dawn\"] = Field(description=\"The time of day in the creative.\")  \n",
    "    weather: Literal[\"None\", \"sunny\", \"snowy\", \"rainy\"] = Field(default=\"None\", description=\"The weather condition in the creative, if identifiable.\")  \n",
    "    background_main_color: str = Field(default=\"None\", description=\"Dominant color of the background.\")  \n",
    "    human_shot_size: List[str] = Field(default=\"None\", description=\"Shot size of the human in the creative (e.g., full shot, medium full shot, cowboy shot, closeup).\")  \n",
    "    human_age: Literal[\"senior\", \"adult\", \"child\", \"baby\"] = Field(default=\"None\", description=\"The age group of the human(s) in the  if they are distinguishable.\")  \n",
    "    electric_vehicle: bool = Field(description=\"Is the ad related to the electrification of the vehicle? True if 'yes', False if 'no'.\")  \n",
    "    goal_awareness_or_consideration_or_conversion: str = Field(default=\"None\", description=\"The ad's purpose: awareness, consideration, or conversion.\") \n",
    "    meta_logo_display: bool = Field(description=\"Is the brand referenced in the first 3 seconds of the video, without considering the logo on the product? True if 'yes', False if 'no'.\")\n",
    "    branding_share: int = Field(None, description=\"Time percentage of the creative where branding (logo, text, etc.) is shown.\")  \n",
    "    black_bars: bool = Field(description=\"Is there a black bar on the creative? True if 'yes', False if 'no'.\")  \n",
    "    text_length: int = Field(default = None, description=\"Number of lines the text takes on the creative without considering the call to action.\") \n",
    "    promotion_display: bool = Field(description=\"Indicates whether a price discount, or price promotion message is displayed in the ad. True if 'yes', False if 'no'.\")  \n",
    "    call_to_action_verb: str = Field(default=\"None\", description=\"The specific verb used in the call to action, if present (e.g., 'buy', 'test').\")  \n",
    "    promotion_wording_type: Literal[\"None\", \"gain\", \"loss_aversion\"] = Field(default=\"None\", description=\"Specifies the type of promotion wording used in the ad: 'gain' for emphasizing benefits, 'loss_aversion' for emphasizing avoidance of loss, or 'None' if there is no promotion wording.\")  \n",
    "    persuasion: Literal[\"None\", \"informative\", \"persuasive\", \"emotional\", \"humorous\", \"inspirational\", \"authoritative\", \"urgent\", \"relatable\"] = Field(default=\"None\", description=\"The tone of voice used for the advertisement, if applicable (e.g., Humour, Surprise, Suspense, Love, Happiness, Neutral).\")  \n",
    "    new_old_vehicle: Literal[\"None\", \"new\", \"second-hand\"] = Field(description=\"Indicates whether the advertised vehicle is new or old.\")  \n",
    "    audio_presence: bool = Field(default=\"None\", description=\"Is there audio (music, voice, ...) in the creative? True if 'yes', False if 'no'.\")  \n",
    "    subtitles_presence: bool = Field(None, description=\"Are there subtitles in the creative? True if 'yes', False if 'no'.\")  \n",
    "    product_usage: bool = Field(description=\"Is the product (car, connected feature, etc.) used in the Ad? Is there a creative of someone driving the car? True if 'yes', False if 'no'.\")  \n",
    "    action_audio: bool = Field(description=\"Is there a clear audio voice that is delivering the call to action? True if 'yes', False if 'no'.\") \n",
    "\n",
    "\n",
    "\n",
    "features_video = []\n",
    "video_analysis_prompt = \"\"\"You are an expert video analyser, describe the video according the the following json schema. Don't take the subtitles of the video into account.\"\"\"\n",
    "\n",
    "def append_features_video():\n",
    "    video_uri = row[\"video_url\"]\n",
    "\n",
    "    contents = [\n",
    "        types.Part.from_uri(\n",
    "            file_uri=video_uri,\n",
    "            mime_type=\"video/*\",\n",
    "        ),\n",
    "        video_analysis_prompt,\n",
    "    ]\n",
    "\n",
    "    response = genai_client.models.generate_content(\n",
    "        model='gemini-2.0-flash-exp', \n",
    "        contents=contents,\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=0,\n",
    "            response_mime_type= 'application/json',\n",
    "            response_schema= VideoInformation\n",
    "        )\n",
    "    )\n",
    "    feature_dict = json.loads(response.text)\n",
    "    feature_dict.update({\"video_url\": row[\"video_url\"]})\n",
    "    features_video.append(feature_dict)\n",
    "    return response\n",
    "\n",
    "\n",
    "for index, row  in tqdm(df_test_video.iterrows(), total=df_test_video.shape[0]):\n",
    "    retry(append_features_video)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = response.usage_metadata.prompt_token_count\n",
    "output_tokens = response.usage_metadata.candidates_token_count\n",
    "intermediate_tokens = response.usage_metadata.cached_content_token_count\n",
    "\n",
    "if intermediate_tokens:\n",
    "    print(\"Cached tokens: \", intermediate_tokens)\n",
    "\n",
    "print(\"Input tokens: \", input_tokens)\n",
    "print(\"Output tokens:\", output_tokens)\n",
    "\n",
    "NUMBER_VIDEO = len(df_test_video)\n",
    "price = input_tokens/1E3*0.00001875 + output_tokens/1E3*0.000075\n",
    "vide_price_per_sec = 0.00002 * 30 #estimation of 30 sec per video\n",
    "print(f\"Total estimated cost for {NUMBER_VIDEO}: ${(price + vide_price_per_sec) * NUMBER_VIDEO}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features Post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of dict into a dataframe\n",
    "output_video_features_df = pd.DataFrame(features_video)\n",
    "output_video_features_df.reset_index(inplace=True, drop=True)\n",
    "output_video_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all text columns to lower case  \n",
    "for col in output_video_features_df.select_dtypes(include='object').columns:  \n",
    "    output_video_features_df[col] = output_video_features_df[col].apply(lambda x: x.lower() if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace occurrences of the string \"None\" with np.nan  \n",
    "output_video_features_df.replace('none', np.nan, inplace=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_video_features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataframe(client, output_video_features_df).to_table(output_video_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Reconstitution of final table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.octocloud import Query\n",
    "\n",
    "sql_image = \"\"\" \n",
    "\n",
    "SELECT\n",
    "  *\n",
    "FROM(\n",
    "  SELECT\n",
    "    *EXCEPT(image_urls),\n",
    "  FROM {image_inputs} LEFT JOIN UNNEST(image_urls) AS image_url \n",
    "  GROUP BY ALL\n",
    ") AS main\n",
    "INNER JOIN(\n",
    "  SELECT\n",
    "    *EXCEPT(id, category)\n",
    "  FROM {image_intermediate}\n",
    ")AS intermediate\n",
    "USING(image_url)\n",
    "INNER JOIN(\n",
    "  SELECT\n",
    "    *EXCEPT(image_url),\n",
    "    COUNT(*) over (PARTITION BY image_hash) AS cnt\n",
    "  FROM {image_outputs}\n",
    "  ORDER BY cnt DESC\n",
    ") AS image_features\n",
    "USING(image_hash)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "query = sql_image.format(\n",
    "    image_inputs=input_ads_table.path(\"standard\"),\n",
    "    image_intermediate=intermediate_image_table.path(\"standard\"),\n",
    "    image_outputs=output_image_table.path(\"standard\")\n",
    ")\n",
    "\n",
    "job = Query(client, query).to_table(features_image_table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_video = \"\"\" \n",
    "\n",
    "SELECT\n",
    "  *\n",
    "FROM(\n",
    "  SELECT\n",
    "    *EXCEPT(image_urls, video_urls),\n",
    "  FROM {video_inputs} LEFT JOIN UNNEST(video_urls) AS video_url\n",
    "  GROUP BY ALL\n",
    ") AS main\n",
    "INNER JOIN(\n",
    "  SELECT\n",
    "    *EXCEPT(id, category)\n",
    "  FROM {video_intermediate}\n",
    "  GROUP BY ALL\n",
    ")AS intermediate\n",
    "USING(video_url)\n",
    "INNER JOIN(\n",
    "  SELECT\n",
    "    *\n",
    "  FROM {video_outputs}\n",
    ") AS features\n",
    "USING(video_url)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "query = sql_video.format(\n",
    "    video_inputs=input_ads_table.path(\"standard\"),\n",
    "    video_intermediate=intermediate_video_table.path(\"standard\"),\n",
    "    video_outputs=output_video_table.path(\"standard\")\n",
    ")\n",
    "# print(query)\n",
    "job = Query(client, query).to_table(features_video_table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
